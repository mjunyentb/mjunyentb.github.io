[{"authors":null,"categories":null,"content":"I am passionate about mobile robotics and AI. I recently finished my PhD on the intersection of AI Planning and Deep Reinforcement Learning at AI \u0026amp; ML group of the Pompeu Fabra University. I am currently looking for a position where I can apply state-of-the-art sequential decision algorithms, preferably in navigation, task and motion planning, while keeping up with the latest research in the field.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am passionate about mobile robotics and AI. I recently finished my PhD on the intersection of AI Planning and Deep Reinforcement Learning at AI \u0026amp; ML group of the Pompeu Fabra University.","tags":null,"title":"Miquel Junyent","type":"authors"},{"authors":["Miquel Junyent","Vicenç Gómez","Anders Jonsson"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635935970,"objectID":"6a77448a106ec4905061f3ea4c9a2468","permalink":"https://mjunyentb.github.io/publication/junyent-2021-hierarchical/","publishdate":"2021-11-03T10:39:30.547365Z","relpermalink":"/publication/junyent-2021-hierarchical/","section":"publication","summary":"Width-based search methods have demonstrated state-of-the-art performance in a wide range of testbeds, from classical planning problems to image-based simulators such as Atari games. These methods scale independently of the size of the state-space, but exponentially in the problem width. In practice, running the algorithm with a width larger than 1 is computationally intractable, prohibiting IW from solving higher width problems. In this paper, we present a hierarchical algorithm that plans at two levels of abstraction. A high-level planner uses abstract features that are incrementally discovered from low-level pruning decisions. We illustrate this algorithm in classical planning PDDL domains as well as in pixel-based simulator domains. In classical planning, we show how IW(1) at two levels of abstraction can solve problems of width 2. For pixel-based domains, we show how in combination with a learned policy and a learned value function, the proposed hierarchical IW can outperform current flat IW-based planners in Atari games with sparse rewards.","tags":[],"title":"Hierarchical Width-Based Planning and Learning","type":"publication"},{"authors":["Miquel Junyent","Anders Jonsson","Vicenç Gómez"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635935970,"objectID":"8cf7931e0502b27225cf438c78d3750a","permalink":"https://mjunyentb.github.io/publication/junyent-2019-deep/","publishdate":"2021-11-03T10:39:30.395725Z","relpermalink":"/publication/junyent-2019-deep/","section":"publication","summary":"Width-based planning has demonstrated great success in recent years due to its ability to scale independently of the size of the state space. For example, Bandres et al. (2018) introduced a rollout version of the Iterated Width algorithm whose performance compares well with humans and learning methods in the pixel setting of the Atari games suite. In this setting, planning is done on-line using the \\\"screen\\\" states and selecting actions by looking ahead into the future. However, this algorithm is purely exploratory and does not leverage past reward information. Furthermore, it requires the state to be factored into features that need to be pre-defined for the particular task, e.g., the B-PROST pixel features. In this work, we extend width-based planning by incorporating an explicit policy in the action selection mechanism. Our method, called π-IW, interleaves width-based planning and policy learning using the state-actions visited by the planner. The policy estimate takes the form of a neural network and is in turn used to guide the planning step, thus reinforcing promising paths. Surprisingly, we observe that the representation learned by the neural network can be used as a feature space for the width-based planner without degrading its performance, thus removing the requirement of pre-defined features for the planner. We compare π-IW with previous width-based methods and with AlphaZero, a method that also interleaves planning and learning, in simple environments, and show that π-IW has superior performance. We also show that π-IW algorithm outperforms previous width-based methods in the pixel setting of Atari games suite.","tags":[],"title":"Deep Policies for Width-Based Planning in Pixel Domains","type":"publication"},{"authors":["Miquel Junyent","Anders Jonsson","Vicenç Gómez"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635938828,"objectID":"755f167b89906fd651a9bf32ac1dbd7a","permalink":"https://mjunyentb.github.io/publication/junyent-2018-improving/","publishdate":"2021-11-03T11:27:08.404945Z","relpermalink":"/publication/junyent-2018-improving/","section":"publication","summary":"Optimal action selection in decision problems characterized by sparse, delayed rewards is still an open challenge. For these problems, current deep reinforcement learning methods require enormous amounts of data to learn controllers that reach human-level performance. In this work, we propose a method that interleaves planning and learning to address this issue. The planning step hinges on the Iterated-Width (IW) planner, a state of the art planner that makes explicit use of the state representation to perform structured exploration. IW is able to scale up to problems independently of the size of the state space. From the state-actions visited by IW, the learning step estimates a compact policy, which in turn is used to guide the planning step. The type of exploration used by our method is radically different than the standard random exploration used in RL. We evaluate our method in simple problems where we show it to have superior performance than the state-of-the-art reinforcement learning algorithms A2C and Alpha Zero. Finally, we present preliminary results in a subset of the Atari games suite.","tags":[],"title":"Improving width-based planning with compact policies","type":"publication"},{"authors":["J De Boer","M Junyent","MR Dijkstra","K Dijkstra","Jaap van de Loosdrecht"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635935970,"objectID":"790d68fc6aa1ae2c654d15dda449ae62","permalink":"https://mjunyentb.github.io/publication/de-2015-twirre/","publishdate":"2021-11-03T10:39:30.684108Z","relpermalink":"/publication/de-2015-twirre/","section":"publication","summary":"Twirre V2 is the evolution of an architecture for mini-UAV platforms which allows automated operation in both GPS-enabled and GPS-deprived applications. The first version of Twirre was implemented and showed good results for indoor navigation, however it had some limitations: the main software components were intertwined and only a limited number of mission states were implemented. This second version separates mission logic, sensor data processing and high-level control, which results in reusable software components for multiple applications. The concept of Local Positioning System (LPS) is introduced, which, using sensor fusion, would aid or automate the flying process like GPS currently does. For this, new sensors are added to the architecture and a generic sensor interface together with missions for landing and following a line have been implemented. V2 introduces a software modular design and new hardware has been coupled, showing its extensibility and adaptability.","tags":[],"title":"Twirre V2: evolution of an architecture for automated mini-UAVs using interchangeable commodity components","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://mjunyentb.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]