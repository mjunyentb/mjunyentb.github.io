[{"authors":null,"categories":null,"content":"As a computer scientist, I am interested in applying AI algorithms to solve sequential decision problems. This interest has grown over the years by developing algorithms for autonomous systems that have to interact with dynamic environments. During my Bachelor thesis, I worked on the autonomous flight of an unmanned aerial vehicle, where I developed a passion for robotics and control that led me to enroll in a MSc in that field. With a more specific interest in a machine learning and optimization approach to the control problem, I decided to pursue a PhD in deep reinforcement learning and AI symbolic planning at AI \u0026amp; ML group of the Pompeu Fabra University, supervised by Anders Jonsson and Vicenç Gómez.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"As a computer scientist, I am interested in applying AI algorithms to solve sequential decision problems. This interest has grown over the years by developing algorithms for autonomous systems that have to interact with dynamic environments.","tags":null,"title":"Miquel Junyent","type":"authors"},{"authors":["Miquel Junyent"],"categories":[],"content":"","date":1634256e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636572191,"objectID":"bcf0804491583704d04172d3c6c5c0dc","permalink":"https://mjunyentb.github.io/publication/junyent-2021-thesis/","publishdate":"2021-11-10T19:23:11.195962Z","relpermalink":"/publication/junyent-2021-thesis/","section":"publication","summary":"Optimal sequential decision making is a fundamental problem to many diverse fields. In recent years, Reinforcement Learning (RL) methods have experienced unprecedented success, largely enabled by the use of deep learning models, reaching human-level performance in several domains, such as the Atari video games or the ancient game of Go. In contrast to the RL approach in which the agent learns a policy from environment interaction samples, ignoring the structure of the problem, the planning approach for decision making assumes known models for the agent's goals and domain dynamics, and focuses on determining how the agent should behave to achieve its objectives. Current planners are able to solve problem instances involving huge state spaces by precisely exploiting the problem structure that is defined in the state-action model.\n\nIn this work we combine the two approaches, leveraging fast and compact policies from learning methods and the capacity to perform  lookaheads in combinatorial problems from planning methods. In particular, we focus on a family of planners called width-based planners, that has demonstrated great success in recent years due to its ability to scale independently of the size of the state space. The basic algorithm, Iterated Width (IW), was originally proposed for classical planning problems, where a model for state transitions and goals, represented by sets of atoms, is fully determined. Nevertheless, width-based planners do not require a fully defined model of the environment, and can be used with simulators. For instance, they have been recently applied in pixel domains such as the Atari games.\n\nDespite its success, IW is purely exploratory, and does not leverage past reward information. Furthermore, it requires the state to be factored into features that need to be pre-defined for the particular task. Moreover, running the algorithm with a width larger than 1 in practice is usually computationally intractable, prohibiting IW from solving higher width problems.\n\nWe begin this dissertation by studying the complexity of width-based methods when the state space is defined by multivalued features, as in the RL setting, instead of Boolean atoms. We provide a tight upper bound on the amount of nodes expanded by IW, as well as overall algorithmic complexity results. In order to deal with more challenging problems (i.e., those with a width higher than 1), we present a hierarchical algorithm that plans at two levels of abstraction. A high-level planner uses abstract features that are incrementally discovered from low-level pruning decisions. We illustrate this algorithm in classical planning PDDL domains as well as in pixel-based simulator domains. In classical planning, we show how IW(1) at two levels of abstraction can solve problems of width 2.\n\nTo leverage past reward information, we extend width-based planning by incorporating an explicit policy in the action selection mechanism. Our method, called π-IW, interleaves width-based planning and policy learning using the state-actions visited by the planner. The policy estimate takes the form of a neural network and is in turn used to guide the planning step, thus reinforcing promising paths. Notably, the representation learned by the neural network can be used as a feature space for the width-based planner without degrading its performance, thus removing the requirement of pre-defined features for the planner. We compare π-IW with previous width-based methods and with AlphaZero, a method that also interleaves planning and learning, in simple environments, and show that π-IW has superior performance. We also show that the π-IW algorithm outperforms previous width-based methods in the pixel setting of Atari games suite. Finally, we show that the proposed hierarchical IW can be seamlessly integrated with our policy learning scheme, resulting in an algorithm that outperforms flat IW-based planners in Atari games with sparse rewards.","tags":[],"title":"Width-Based Planning and Learning","type":"publication"},{"authors":["Miquel Junyent","Vicenç Gómez","Anders Jonsson"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635935970,"objectID":"6a77448a106ec4905061f3ea4c9a2468","permalink":"https://mjunyentb.github.io/publication/junyent-2021-hierarchical/","publishdate":"2021-11-03T10:39:30.547365Z","relpermalink":"/publication/junyent-2021-hierarchical/","section":"publication","summary":"Width-based search methods have demonstrated state-of-the-art performance in a wide range of testbeds, from classical planning problems to image-based simulators such as Atari games. These methods scale independently of the size of the state-space, but exponentially in the problem width. In practice, running the algorithm with a width larger than 1 is computationally intractable, prohibiting IW from solving higher width problems. In this paper, we present a hierarchical algorithm that plans at two levels of abstraction. A high-level planner uses abstract features that are incrementally discovered from low-level pruning decisions. We illustrate this algorithm in classical planning PDDL domains as well as in pixel-based simulator domains. In classical planning, we show how IW(1) at two levels of abstraction can solve problems of width 2. For pixel-based domains, we show how in combination with a learned policy and a learned value function, the proposed hierarchical IW can outperform current flat IW-based planners in Atari games with sparse rewards.","tags":[],"title":"Hierarchical Width-Based Planning and Learning","type":"publication"},{"authors":["Miquel Junyent","Anders Jonsson","Vicenç Gómez"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635935970,"objectID":"8cf7931e0502b27225cf438c78d3750a","permalink":"https://mjunyentb.github.io/publication/junyent-2019-deep/","publishdate":"2021-11-03T10:39:30.395725Z","relpermalink":"/publication/junyent-2019-deep/","section":"publication","summary":"Width-based planning has demonstrated great success in recent years due to its ability to scale independently of the size of the state space. For example, Bandres et al. (2018) introduced a rollout version of the Iterated Width algorithm whose performance compares well with humans and learning methods in the pixel setting of the Atari games suite. In this setting, planning is done on-line using the \\\"screen\\\" states and selecting actions by looking ahead into the future. However, this algorithm is purely exploratory and does not leverage past reward information. Furthermore, it requires the state to be factored into features that need to be pre-defined for the particular task, e.g., the B-PROST pixel features. In this work, we extend width-based planning by incorporating an explicit policy in the action selection mechanism. Our method, called π-IW, interleaves width-based planning and policy learning using the state-actions visited by the planner. The policy estimate takes the form of a neural network and is in turn used to guide the planning step, thus reinforcing promising paths. Surprisingly, we observe that the representation learned by the neural network can be used as a feature space for the width-based planner without degrading its performance, thus removing the requirement of pre-defined features for the planner. We compare π-IW with previous width-based methods and with AlphaZero, a method that also interleaves planning and learning, in simple environments, and show that π-IW has superior performance. We also show that π-IW algorithm outperforms previous width-based methods in the pixel setting of Atari games suite.","tags":[],"title":"Deep Policies for Width-Based Planning in Pixel Domains","type":"publication"},{"authors":["Miquel Junyent","Anders Jonsson","Vicenç Gómez"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635938828,"objectID":"755f167b89906fd651a9bf32ac1dbd7a","permalink":"https://mjunyentb.github.io/publication/junyent-2018-improving/","publishdate":"2021-11-03T11:27:08.404945Z","relpermalink":"/publication/junyent-2018-improving/","section":"publication","summary":"Optimal action selection in decision problems characterized by sparse, delayed rewards is still an open challenge. For these problems, current deep reinforcement learning methods require enormous amounts of data to learn controllers that reach human-level performance. In this work, we propose a method that interleaves planning and learning to address this issue. The planning step hinges on the Iterated-Width (IW) planner, a state of the art planner that makes explicit use of the state representation to perform structured exploration. IW is able to scale up to problems independently of the size of the state space. From the state-actions visited by IW, the learning step estimates a compact policy, which in turn is used to guide the planning step. The type of exploration used by our method is radically different than the standard random exploration used in RL. We evaluate our method in simple problems where we show it to have superior performance than the state-of-the-art reinforcement learning algorithms A2C and Alpha Zero. Finally, we present preliminary results in a subset of the Atari games suite.","tags":[],"title":"Improving width-based planning with compact policies","type":"publication"},{"authors":["J De Boer","M Junyent","MR Dijkstra","K Dijkstra","Jaap van de Loosdrecht"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635935970,"objectID":"790d68fc6aa1ae2c654d15dda449ae62","permalink":"https://mjunyentb.github.io/publication/de-2015-twirre/","publishdate":"2021-11-03T10:39:30.684108Z","relpermalink":"/publication/de-2015-twirre/","section":"publication","summary":"Twirre V2 is the evolution of an architecture for mini-UAV platforms which allows automated operation in both GPS-enabled and GPS-deprived applications. The first version of Twirre was implemented and showed good results for indoor navigation, however it had some limitations: the main software components were intertwined and only a limited number of mission states were implemented. This second version separates mission logic, sensor data processing and high-level control, which results in reusable software components for multiple applications. The concept of Local Positioning System (LPS) is introduced, which, using sensor fusion, would aid or automate the flying process like GPS currently does. For this, new sensors are added to the architecture and a generic sensor interface together with missions for landing and following a line have been implemented. V2 introduces a software modular design and new hardware has been coupled, showing its extensibility and adaptability.","tags":[],"title":"Twirre V2: evolution of an architecture for automated mini-UAVs using interchangeable commodity components","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://mjunyentb.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]